![header](https://capsule-render.vercel.app/api?type=slice&color=gradient&height=200&section=footer&text=BERT%20PROJECT&fontSize=100)
 

## 📌 1. 프로젝트 개요

금융 뉴스 헤드라인을 보고 감정을 분류하는 모델을 만들어보자는 아이디어로 프로젝트를 시작했다.  
처음에는 약 10만 개의 원본 데이터를 기반으로 모델을 학습하고 평가할 계획이었지만,  
실제로 데이터를 살펴보니 문장 품질이나 라벨 정합성이 너무 낮아, 이걸 그대로 사용할 수는 없다고 판단했다.

그래서 원본 데이터를 신뢰할 수 없다고 판단하고,  
**내가 직접 문장을 수집하고 라벨링하는 방식으로 방향을 전환**했다.  
최종적으로 **총 3,300개의 뉴스 헤드라인을 수집**하고,  
**부정(0), 긍정(1), 중립(2)** 세 클래스로 **수작업 라벨링을 진행**했다.

감정 판단 기준은 단순 키워드가 아니라 **금융적인 맥락과 문장 흐름**을 중심으로 잡았으며,  
클래스 간 데이터 불균형이 없도록 **세 클래스를 균형 있게 구성**했다.

이후, **이렇게 학습된 모델을 이용해 원본 데이터를 다시 평가**해보기로 했다.  
그 과정에서 **직접 라벨링한 3클래스 데이터에서 중립(2)을 제외한 0/1 데이터만으로 MobileBERT를 학습**한 뒤,  
30k 슬라이싱 원본 데이터에 예측을 수행해보았다.  
그 결과, validation 기준으로는 **92%의 높은 정확도**를 보였지만,  
중립이 포함된 실제 원본 데이터셋에서는 **정확도가 27%로 급락**했다.  
이 실험을 통해, **중립 클래스를 제거하면 모델이 감정 판단 기준 자체를 상실**하고,  
현실 데이터를 해석하지 못한다는 결론에 도달했다.

그 후 이 데이터를 바탕으로,  
MobileBERT, FinBERT, DeBERTa 세 가지 BERT 계열 모델을 fine-tuning 하고  
직접 라벨링한 데이터셋에 대한 성능을 비교 평가했다.

마지막으로, 모델이 자주 틀리는 문장들을 모아 원본 데이터셋이 틀린 것이   
각각의 모델들이 틀린 토픽들과 연관성이 있는지   
역추적하기 위해 BERTopic 기반 토픽 모델링을 수행했다.  
이 과정을 통해 모델이 어려워하는 토픽과 모델별 성향 차이를 파악하고,  
뉴스 데이터에서 감정 인식이 까다로운 주제들이 무엇인지 정성적으로 분석할 수 있었다.




## 📌 2. 데이터 라벨링 기준 및 직접 라벨링한 데이터 구성

원본 데이터를 분석해보면서 라벨이 너무 믿기 어렵다고 판단한 이후,  
아예 내가 직접 데이터를 라벨링하는 방식으로 방향을 바꿨다.  
처음엔 귀찮고 시간이 오래 걸릴 것 같아서 고민도 했지만,  
결국 좋은 모델을 만들려면 정확한 기준으로 만들어진 데이터가 중요하다고 판단해서 수작업 라벨링을 진행했다.

총 약 **3,300개의 문장**을 라벨링했고, 감정은 **부정(0), 긍정(1), 중립(2)** 이렇게 세 가지 클래스(Class)로 나눴다.

### 🔹 라벨링 기준

- **0 (부정)**  
  주가 하락, 실적 악화, 구조조정, 법적 리스크 등 부정적 뉴스  
  예: 
`"국제 전자 산업 회사 엘코텍은 탈린 시설에서 수십 명의 직원을 해고했으며"`
`"영업 손실은 총 0.3백만 유로로 2007년 같은 기간의 220만 유로 이익에 비해 크게 증가했습니다."`

- **1 (긍정)**  
  주가 상승 기대, 실적 개선, 투자 유치, 낙관적 전망 등 긍정적 뉴스  
  예: `"테슬라, 전기차 배터리 수주 대폭 확대"` 
  `"새로운 생산 공장을 통해 회사는 예상되는 수요 증가를 충족할 수 있는"`

- **2 (중립)**  
  감정이 없는 단순 사실 전달 또는 판단이 애매한 경우  
  예: `"NVIDIA 주가 변동폭 커"`, `"이번 거래는 경직된 플라스틱 소비재 운영에 대한 허타마키의 전략적 검토를 뒷받침합니다."라고 허탐+횈키 오이지의 CEO인 죽카 모이시오(Jukka Moisio)는 말합니다"`

### 🔹 라벨링 시 고려한 점

- 단어 위주 판단이 아닌 **문맥과 금융적 의미** 중심으로 분류했음(중립처리 때문임.)
- 긍/부정이 아닌 정보만을 나열하거나 애매한 문장은 **중립** 처리했음. 
- 스스로 기준이 흔들리지 않게 중간중간 **기준 점검했음.**

- ### 🔹 라벨 분포 요약

| 라벨 | 의미   | 개수 (대략) |
|------|--------|-------------|
| 0    | 부정   | 약 1,100건   |
| 1    | 긍정   | 약 1,100건   |
| 2    | 중립   | 약 1,100건   |

> ⚖️ 클래스 밸런스를 최대한 맞춰서 모델이 편향 없이 학습되도록 구성함

---
##  📊 모델 성능 비교
### ⚠️ 초기 중립 제거 모델 성능 비교

아래는 **중립 클래스를 제거한 0/1 이진 분류 MobileBERT 모델**이  
직접 라벨링한 데이터(Validation)와 **원본 슬라이싱 데이터(30k)**에서 각각 얼마나 성능을 보였는지를 비교한 표이다.

| Model         | Val Accuracy (직접 라벨링 데이터) | Accuracy (원본 30k 데이터 평가) |
|---------------|------------------------------------|-----------------------------------|
| MobileBERT (중립 제거) | **92.00%**                            | **27.00%**                          |

> ⚠️ *Validation에서는 높은 성능을 보였지만, 실제 중립이 포함된 현실 데이터에 대해선 극단적으로 낮은 성능을 보이며,  
> 중립 제거 방식이 실전에서는 부적절하다는 것을 보여주는 중요한 실험 결과를 알 수 있었다.*

---
### 현재 사용 모델:

| Model     | Val Accuracy |
|-----------|--------------|
| MobileBERT | 85.63%       |
| FinBERT    | 86.28%       |
| DeBERTa    | 88.05%       |

> ✅ *DeBERTa가 가장 높은 정확도를 보였으며, 세 모델 간 성능 차이는 크지 않지만 미세한 차이가 존재함.*
---


## 📌 4. 슬라이싱 데이터 재라벨링 및 정합성 평가

슬라이싱한 약 30,000개의 원본 데이터를 대상으로, 학습된 모델(MobileBERT, FinBERT, DeBERTa)로 예측을 수행했다.  
이 과정에서 모델이 **중립(2)** 으로 예측한 비율이 모델마다 다르게 나타났고, 이 차이가 이후 평가 가능한 데이터 수와 정확도에 영향을 주게 된다.

---

### 🧩 모델별 "중립" 분류 비율 차이

| Model     | 전체 예측 수 | 중립으로 분류된 수 | 중립 비율 (%) |
|-----------|---------------|----------------------|----------------|
| MobileBERT | 약 30,000     | 20,529               | 68.43%         |
| FinBERT    | 약 30,000     | 22,900               | 76.33%         |
| DeBERTa    | 약 30,000     | 20,102               | 67.01%         |

> 📌 *중립 라벨을 가장 많이 예측한 모델은 FinBERT였고, 반대로 MobileBERT와 DeBERTa는 상대적으로 중립 비율이 낮았다.*

이 차이는 **모델이 '중립'을 인식하는 기준이나 민감도가 다르다는 걸 보여준다.**  
예를 들어, FinBERT는 금융 도메인에 특화된 모델이기 때문에 **보수적으로 감정을 판단**하는 경향이 있을 수 있고,  
MobileBERT는 경량화된 구조로 인해 맥락 판단이 단순화되어 **모호한 문장을 긍/부정으로 분류할 확률이 높다.**

이런 차이로 인해, **모델마다 중립 제외 후 사용 가능한 평가 샘플 수**도 달라지며,  
이것이 최종 정확도에도 영향을 미친다.

---

### 📊 중립 제거 후 2진 분류 정합성 평가 (원본 라벨 기준 Accuracy)

중립(2)으로 분류된 행을 제거하고 남은 긍정/부정 예측 결과만을,  
기존 원본 라벨과 비교해 Accuracy를 측정했다.

| Model     | Accuracy (중립 제외 후) | 평가 샘플 수 |
|-----------|--------------------------|---------------|
| MobileBERT | 52.70%                   | 9,471         |
| FinBERT    | 54.96%                   | 7,100         |
| DeBERTa    | 52.94%                   | 9,898         |

> ⚠️ *모델 성능이 낮다기보다, 원본 라벨과 모델 간 기준 차이가 많이 나고 원본 데이터셋이 좋지않음을 알 수 있음.*

---



5. 중립 제거 시 발생하는 문제점 논의
의문: 중립(2) 라벨 제거 후 0/1 분류로 모델 재학습 시, 원본 데이터의 정확도 저하 가능성?

분석:

실제 실험 결과, 중립 제거 후 모델 정확도는 약 52~55% 수준으로 하락

이유: 원본 뉴스에서 '중립적인 문장'이 실제로 매우 많은 비중을 차지하고 있었기 때문

결론: 중립 제거 모델은 원본 데이터의 감정 다양성을 충분히 반영하지 못함

6. 오류 분석 및 토픽 모델링
오류 데이터 추출 방법:

모델별 예측값과 직접 라벨 불일치 행 추출

BERTopic 기반 토픽 모델링 수행

모델별로 잘못 분류된 문장의 토픽 분포 확인

잘 분류되는 토픽 vs. 못 분류되는 토픽 비교

활용:

원본 30k 데이터도 동일하게 BERTopic 분석

오류 데이터와의 토픽 중첩 여부 확인 → 모델 성능 저하 원인 분석

모델 간 성능 차이 분석 및 특성 파악

7. Confusion Matrix 및 정량 평가
모델별 confusion matrix 및 classification report 시각화

FN/FP 비율 분석 → 모델이 민감하게 반응하는 케이스 분석 가능

8. 실패 사례 정성 분석
실제 예시 제시:

중립적인 문장이 잘못 긍/부정으로 분류됨

부정적 사건이 긍정으로 분류된 사례 (예: 해고, 구조조정 → 긍정)

시사점:

BERT 모델은 문맥 기반이지만, 금융 도메인에서는 단어 선택이 매우 결정적

뉴스에서의 부정적 워딩은 맥락보다 직접적 의미로 분류되는 경우 많음

9. 결론 및 제언
중립 클래스의 중요성: 중립 라벨 제거는 현실적인 감정 분류 성능을 크게 저하시키므로, 향후 모델 학습 시 반드시 유지 필요

모델 선택: DeBERTa가 가장 안정적이며 성능 우수

향후 과제: 더 많은 수작업 라벨링 또는 weak supervision 방식 도입 고려

